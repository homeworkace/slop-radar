{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890c483-8fcb-4b7b-a545-f7ef774edefd",
   "metadata": {},
   "source": [
    "# Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b8be1a6-8630-4a97-b05c-1aa2e18a6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset.json'\n",
    "batch_dataset_path = 'batch_dataset.json'\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c239a68b-1f68-4550-a536-5343688fe180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.26.5)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Homeworkace\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (3.13.3)\n",
      "Requirement already satisfied: requests in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Requirement already satisfied: datasets in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: filelock in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.23.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Homeworkace\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.23.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Homeworkace\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.23.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\homeworkace\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Homeworkace\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install datasets\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3ba1c-efc1-461f-98e2-9f98aa943001",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a9727e-86e4-4b5f-a2bf-3b2a29b8716e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if Path(dataset_path).exists() :\n",
    "    dataset = pd.read_json(dataset_path)\n",
    "else :\n",
    "    notebook_login()\n",
    "    hugging_face_dataset = datasets.load_dataset('lmsys/chatbot_arena_conversations')\n",
    "    dataset = hugging_face_dataset['train'].to_pandas()\n",
    "    dataset.to_json(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25810d5d-4f05-45f7-b69d-75b16ac4d49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>judge</th>\n",
       "      <th>conversation_a</th>\n",
       "      <th>conversation_b</th>\n",
       "      <th>turn</th>\n",
       "      <th>anony</th>\n",
       "      <th>language</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>openai_moderation</th>\n",
       "      <th>toxic_chat_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8120899314f74641b09c2aa114d4d253</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>model_b</td>\n",
       "      <td>arena_user_316</td>\n",
       "      <td>[{'content': 'Salut ! Comment ça va ce matin ?...</td>\n",
       "      <td>[{'content': 'Salut ! Comment ça va ce matin ?...</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>French</td>\n",
       "      <td>1.682354e+09</td>\n",
       "      <td>{'categories': {'harassment': False, 'harassme...</td>\n",
       "      <td>{'roberta-large': {'flagged': False, 'probabil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question_id     model_a     model_b   winner  \\\n",
       "33  8120899314f74641b09c2aa114d4d253  alpaca-13b  vicuna-13b  model_b   \n",
       "\n",
       "             judge                                     conversation_a  \\\n",
       "33  arena_user_316  [{'content': 'Salut ! Comment ça va ce matin ?...   \n",
       "\n",
       "                                       conversation_b  turn  anony language  \\\n",
       "33  [{'content': 'Salut ! Comment ça va ce matin ?...     6   True   French   \n",
       "\n",
       "          tstamp                                  openai_moderation  \\\n",
       "33  1.682354e+09  {'categories': {'harassment': False, 'harassme...   \n",
       "\n",
       "                                       toxic_chat_tag  \n",
       "33  {'roberta-large': {'flagged': False, 'probabil...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['turn'] > 3][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cff15e-f869-42f2-8e2b-df557967b169",
   "metadata": {},
   "source": [
    "# Create text-author pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29aeca5-eec7-476e-9480-826fe4589729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list for processed data\n",
    "text_author_pairs = []\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for _, row in dataset.iterrows():\n",
    "    # Process conversation_a (assistant role)\n",
    "    for message in row[\"conversation_a\"]:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            text_author_pairs.append({\n",
    "                \"text\": message[\"content\"],\n",
    "                \"author\": row[\"model_a\"]\n",
    "            })\n",
    "\n",
    "    # Process conversation_b (assistant role)\n",
    "    for message in row[\"conversation_b\"]:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            text_author_pairs.append({\n",
    "                \"text\": message[\"content\"],\n",
    "                \"author\": row[\"model_b\"]\n",
    "            })\n",
    "\n",
    "    # Process user messages (common across both conversations)\n",
    "    for message in row[\"conversation_a\"]:  # Check only `conversation_a` since user messages are identical\n",
    "        if message[\"role\"] == \"user\":\n",
    "            text_author_pairs.append({\n",
    "                \"text\": message[\"content\"],\n",
    "                \"author\": \"human\"\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779269b-827d-44a0-842a-7007c968ace0",
   "metadata": {},
   "source": [
    "# Create token count vectors for each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "68220c4b-c0da-4024-8ee0-cf878edcc662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 117948/117948 [01:21<00:00, 1442.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting into DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 591/591 [00:00<00:00, 2221.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_chatglm-6b</th>\n",
       "      <th>0_koala-13b</th>\n",
       "      <th>0_human</th>\n",
       "      <th>0_oasst-pythia-12b</th>\n",
       "      <th>0_alpaca-13b</th>\n",
       "      <th>0_vicuna-13b</th>\n",
       "      <th>0_dolly-v2-12b</th>\n",
       "      <th>0_stablelm-tuned-alpha-7b</th>\n",
       "      <th>0_llama-13b</th>\n",
       "      <th>1_human</th>\n",
       "      <th>...</th>\n",
       "      <th>190_human</th>\n",
       "      <th>191_human</th>\n",
       "      <th>192_human</th>\n",
       "      <th>6_wizardlm-13b</th>\n",
       "      <th>193_human</th>\n",
       "      <th>6_gpt4all-13b-snoozy</th>\n",
       "      <th>5_guanaco-33b</th>\n",
       "      <th>194_human</th>\n",
       "      <th>195_human</th>\n",
       "      <th>196_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ｔｏ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ｗｅｅｋｓ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝗶𝗺𝗽𝗹𝗲</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195411 rows × 591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0_chatglm-6b  0_koala-13b  0_human  0_oasst-pythia-12b  \\\n",
       "00                      0            0        0                   0   \n",
       "000                     3            1        0                   4   \n",
       "0000                    0            0        0                   0   \n",
       "00000                   0            0        0                   0   \n",
       "000000                  0            2        0                   0   \n",
       "...                   ...          ...      ...                 ...   \n",
       "ｔｏ                      0            0        0                   0   \n",
       "ｗｅｅｋｓ                   0            0        0                   0   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                  0            0        0                   0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻              0            0        0                   0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀             0            0        0                   0   \n",
       "\n",
       "             0_alpaca-13b  0_vicuna-13b  0_dolly-v2-12b  \\\n",
       "00                      8             3               1   \n",
       "000                     0             2               4   \n",
       "0000                    0             0               0   \n",
       "00000                   0             0               0   \n",
       "000000                  0             0               0   \n",
       "...                   ...           ...             ...   \n",
       "ｔｏ                      0             0               0   \n",
       "ｗｅｅｋｓ                   0             0               0   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                  0             0               0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻              0             0               0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀             0             0               0   \n",
       "\n",
       "             0_stablelm-tuned-alpha-7b  0_llama-13b  1_human  ...  190_human  \\\n",
       "00                                   0            1        3  ...          0   \n",
       "000                                  0            0        0  ...          0   \n",
       "0000                                 0            0        0  ...          0   \n",
       "00000                                0            0        0  ...          0   \n",
       "000000                               0            0        0  ...          0   \n",
       "...                                ...          ...      ...  ...        ...   \n",
       "ｔｏ                                   0            0        0  ...          0   \n",
       "ｗｅｅｋｓ                                0            0        0  ...          0   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                               0            0        0  ...          0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻                           0            0        0  ...          0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀                          0            0        0  ...          0   \n",
       "\n",
       "             191_human  192_human  6_wizardlm-13b  193_human  \\\n",
       "00                   0          0               3          0   \n",
       "000                  0          3               6          0   \n",
       "0000                 0          0               0          0   \n",
       "00000                0          0               0          0   \n",
       "000000               0          0               0          0   \n",
       "...                ...        ...             ...        ...   \n",
       "ｔｏ                   0          0               0          0   \n",
       "ｗｅｅｋｓ                0          0               0          0   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲               0          0               0          0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻           0          0               0          0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀          0          0               0          0   \n",
       "\n",
       "             6_gpt4all-13b-snoozy  5_guanaco-33b  194_human  195_human  \\\n",
       "00                              0              0          0          0   \n",
       "000                             3              0          7          2   \n",
       "0000                            0              0          0          0   \n",
       "00000                           0              0          0          0   \n",
       "000000                          0              0          0          0   \n",
       "...                           ...            ...        ...        ...   \n",
       "ｔｏ                              0              0          0          0   \n",
       "ｗｅｅｋｓ                           0              0          0          0   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                          0              0          0          0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻                      0              0          0          0   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀                     0              0          0          0   \n",
       "\n",
       "             196_human  \n",
       "00                   2  \n",
       "000                  0  \n",
       "0000                 0  \n",
       "00000                0  \n",
       "000000               0  \n",
       "...                ...  \n",
       "ｔｏ                   0  \n",
       "ｗｅｅｋｓ                0  \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲               0  \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻           0  \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀          0  \n",
       "\n",
       "[195411 rows x 591 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract texts and their authors\n",
    "texts = [entry['text'] for entry in text_author_pairs]\n",
    "authors = [entry['author'] for entry in text_author_pairs]\n",
    "\n",
    "# Vectorize the paragraphs\n",
    "vectoriser = CountVectorizer()\n",
    "text_vectors = vectoriser.fit_transform(texts)  # Each row is a text\n",
    "\n",
    "# Allocate quotas for each batch.\n",
    "batch_count = {} \n",
    "for author in list(set(authors)) :\n",
    "    # Find out how many batches of 200 texts are needed.\n",
    "    number_of_batches = round(authors.count(author) / 200)\n",
    "    for i in range(number_of_batches) :\n",
    "        # Distribute the remainder/shortfall evenly.\n",
    "        batch_count[str(i) + '_' + author] = np.floor((i + 1) * authors.count(author) / number_of_batches)\n",
    "        for j in range(i) :\n",
    "            # Subtract the count of previous batches.\n",
    "            batch_count[str(i) + '_' + author] -= batch_count[str(j) + '_' + author]\n",
    "        \n",
    "print('Populating batches...')\n",
    "batch_vectors = {}\n",
    "batch_quota = batch_count.copy()\n",
    "for i, author in enumerate(tqdm(authors)):\n",
    "    batch_to_insert = [name for name in batch_quota if author in name and batch_quota[name] > 0][0]\n",
    "    if batch_to_insert in batch_vectors:\n",
    "        # The resulting vectors represent the number of times each token appears in that author's entire corpus.\n",
    "        batch_vectors[batch_to_insert] += text_vectors[i].toarray()\n",
    "    else:\n",
    "        # Add this new batch to the list.\n",
    "        batch_vectors[batch_to_insert] = text_vectors[i].toarray()\n",
    "    batch_quota[batch_to_insert] -= 1\n",
    "\n",
    "# Convert to a dataframe\n",
    "print('Converting into DataFrame...')\n",
    "batch_dataset = pd.DataFrame({name: vector.flatten() for name, vector in tqdm(batch_vectors.items())}, index = vectoriser.get_feature_names_out())\n",
    "\n",
    "batch_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddb289-5636-47db-a7c2-fe29843d88c5",
   "metadata": {},
   "source": [
    "# Group texts by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "905cb5a4-0217-43e1-b377-a18c18b7074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalising...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 591/591 [01:13<00:00,  8.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_chatglm-6b</th>\n",
       "      <th>0_koala-13b</th>\n",
       "      <th>0_human</th>\n",
       "      <th>0_oasst-pythia-12b</th>\n",
       "      <th>0_alpaca-13b</th>\n",
       "      <th>0_vicuna-13b</th>\n",
       "      <th>0_dolly-v2-12b</th>\n",
       "      <th>0_stablelm-tuned-alpha-7b</th>\n",
       "      <th>0_llama-13b</th>\n",
       "      <th>1_human</th>\n",
       "      <th>...</th>\n",
       "      <th>190_human</th>\n",
       "      <th>191_human</th>\n",
       "      <th>192_human</th>\n",
       "      <th>6_wizardlm-13b</th>\n",
       "      <th>193_human</th>\n",
       "      <th>6_gpt4all-13b-snoozy</th>\n",
       "      <th>5_guanaco-33b</th>\n",
       "      <th>194_human</th>\n",
       "      <th>195_human</th>\n",
       "      <th>196_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ｗｅｅｋｓ</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝗶𝗺𝗽𝗹𝗲</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>158.492462</td>\n",
       "      <td>121.745000</td>\n",
       "      <td>14.557789</td>\n",
       "      <td>99.839196</td>\n",
       "      <td>43.432161</td>\n",
       "      <td>111.603015</td>\n",
       "      <td>62.216495</td>\n",
       "      <td>121.950739</td>\n",
       "      <td>53.125628</td>\n",
       "      <td>19.855000</td>\n",
       "      <td>...</td>\n",
       "      <td>19.28</td>\n",
       "      <td>33.865</td>\n",
       "      <td>24.366834</td>\n",
       "      <td>121.567839</td>\n",
       "      <td>23.145</td>\n",
       "      <td>93.529412</td>\n",
       "      <td>151.721154</td>\n",
       "      <td>37.452261</td>\n",
       "      <td>22.035000</td>\n",
       "      <td>41.570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195412 rows × 591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0_chatglm-6b  0_koala-13b    0_human  0_oasst-pythia-12b  \\\n",
       "00               0.000000     0.000000   0.000000            0.000000   \n",
       "000              0.000095     0.000041   0.000000            0.000201   \n",
       "0000             0.000000     0.000000   0.000000            0.000000   \n",
       "00000            0.000000     0.000000   0.000000            0.000000   \n",
       "000000           0.000000     0.000082   0.000000            0.000000   \n",
       "...                   ...          ...        ...                 ...   \n",
       "ｗｅｅｋｓ            0.000000     0.000000   0.000000            0.000000   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲           0.000000     0.000000   0.000000            0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻       0.000000     0.000000   0.000000            0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀      0.000000     0.000000   0.000000            0.000000   \n",
       " length        158.492462   121.745000  14.557789           99.839196   \n",
       "\n",
       "             0_alpaca-13b  0_vicuna-13b  0_dolly-v2-12b  \\\n",
       "00               0.000926      0.000135        0.000083   \n",
       "000              0.000000      0.000090        0.000331   \n",
       "0000             0.000000      0.000000        0.000000   \n",
       "00000            0.000000      0.000000        0.000000   \n",
       "000000           0.000000      0.000000        0.000000   \n",
       "...                   ...           ...             ...   \n",
       "ｗｅｅｋｓ            0.000000      0.000000        0.000000   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲           0.000000      0.000000        0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻       0.000000      0.000000        0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀      0.000000      0.000000        0.000000   \n",
       " length         43.432161    111.603015       62.216495   \n",
       "\n",
       "             0_stablelm-tuned-alpha-7b  0_llama-13b    1_human  ...  \\\n",
       "00                            0.000000     0.000095   0.000755  ...   \n",
       "000                           0.000000     0.000000   0.000000  ...   \n",
       "0000                          0.000000     0.000000   0.000000  ...   \n",
       "00000                         0.000000     0.000000   0.000000  ...   \n",
       "000000                        0.000000     0.000000   0.000000  ...   \n",
       "...                                ...          ...        ...  ...   \n",
       "ｗｅｅｋｓ                         0.000000     0.000000   0.000000  ...   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                        0.000000     0.000000   0.000000  ...   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻                    0.000000     0.000000   0.000000  ...   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀                   0.000000     0.000000   0.000000  ...   \n",
       " length                     121.950739    53.125628  19.855000  ...   \n",
       "\n",
       "             190_human  191_human  192_human  6_wizardlm-13b  193_human  \\\n",
       "00                0.00      0.000   0.000000        0.000124      0.000   \n",
       "000               0.00      0.000   0.000619        0.000248      0.000   \n",
       "0000              0.00      0.000   0.000000        0.000000      0.000   \n",
       "00000             0.00      0.000   0.000000        0.000000      0.000   \n",
       "000000            0.00      0.000   0.000000        0.000000      0.000   \n",
       "...                ...        ...        ...             ...        ...   \n",
       "ｗｅｅｋｓ             0.00      0.000   0.000000        0.000000      0.000   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲            0.00      0.000   0.000000        0.000000      0.000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻        0.00      0.000   0.000000        0.000000      0.000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀       0.00      0.000   0.000000        0.000000      0.000   \n",
       " length          19.28     33.865  24.366834      121.567839     23.145   \n",
       "\n",
       "             6_gpt4all-13b-snoozy  5_guanaco-33b  194_human  195_human  \\\n",
       "00                       0.000000       0.000000   0.000000   0.000000   \n",
       "000                      0.000157       0.000000   0.000939   0.000454   \n",
       "0000                     0.000000       0.000000   0.000000   0.000000   \n",
       "00000                    0.000000       0.000000   0.000000   0.000000   \n",
       "000000                   0.000000       0.000000   0.000000   0.000000   \n",
       "...                           ...            ...        ...        ...   \n",
       "ｗｅｅｋｓ                    0.000000       0.000000   0.000000   0.000000   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                   0.000000       0.000000   0.000000   0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻               0.000000       0.000000   0.000000   0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀              0.000000       0.000000   0.000000   0.000000   \n",
       " length                 93.529412     151.721154  37.452261  22.035000   \n",
       "\n",
       "             196_human  \n",
       "00            0.000241  \n",
       "000           0.000000  \n",
       "0000          0.000000  \n",
       "00000         0.000000  \n",
       "000000        0.000000  \n",
       "...                ...  \n",
       "ｗｅｅｋｓ         0.000000  \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲        0.000000  \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻    0.000000  \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀   0.000000  \n",
       " length      41.570000  \n",
       "\n",
       "[195412 rows x 591 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the average text length of each author. \n",
    "batch_average_length = {name: np.sum(vector) / batch_count[name] for name, vector in batch_vectors.items()}\n",
    "\n",
    "# Normalise the length of each corpus.\n",
    "print('Normalising...')\n",
    "for name in tqdm(batch_vectors) :\n",
    "    batch_dataset[name] /= np.sum(batch_vectors[name])\n",
    "    \n",
    "# Since it has been lost, add the average text length as a separate feature.\n",
    "batch_dataset = pd.concat([batch_dataset, pd.DataFrame(batch_average_length, index = [' length'])])\n",
    "\n",
    "batch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "349b0638-7c11-4a20-809b-a1524a765f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dataset.to_json(batch_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8da3ae-4656-4e4b-a51d-5215e7ffa0fe",
   "metadata": {},
   "source": [
    "# Custom holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a711fed7-9d46-4ffd-a246-aeebd52f2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout(df, authors) :\n",
    "    training_df = df.copy()\n",
    "    holdout_list = []\n",
    "    for author in authors :\n",
    "        batch = training_df[random.choice([batch for batch in training_df.columns if author in batch])]\n",
    "        training_df.drop(batch.name, axis = 1)\n",
    "        holdout_list.append(batch)\n",
    "\n",
    "    holdout_df = pd.concat(holdout_list, axis = 1)\n",
    "    return training_df, holdout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3ea2a6f3-b120-4248-98fc-e31993382d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>27_gpt-3.5-turbo</th>\n",
       "      <th>12_vicuna-7b</th>\n",
       "      <th>13_chatglm-6b</th>\n",
       "      <th>1_alpaca-13b</th>\n",
       "      <th>8_mpt-7b-chat</th>\n",
       "      <th>15_stablelm-tuned-alpha-7b</th>\n",
       "      <th>12_gpt-4</th>\n",
       "      <th>25_oasst-pythia-12b</th>\n",
       "      <th>9_claude-v1</th>\n",
       "      <th>30_koala-13b</th>\n",
       "      <th>...</th>\n",
       "      <th>4_gpt4all-13b-snoozy</th>\n",
       "      <th>3_llama-13b</th>\n",
       "      <th>32_vicuna-13b</th>\n",
       "      <th>4_claude-instant-v1</th>\n",
       "      <th>9_dolly-v2-12b</th>\n",
       "      <th>4_RWKV-4-Raven-14B</th>\n",
       "      <th>0_guanaco-33b</th>\n",
       "      <th>8_palm-2</th>\n",
       "      <th>17_fastchat-t5-3b</th>\n",
       "      <th>5_wizardlm-13b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ｗｅｅｋｓ</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝗶𝗺𝗽𝗹𝗲</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>106.535</td>\n",
       "      <td>113.361809</td>\n",
       "      <td>148.495000</td>\n",
       "      <td>38.325000</td>\n",
       "      <td>83.795918</td>\n",
       "      <td>111.465686</td>\n",
       "      <td>142.040404</td>\n",
       "      <td>81.562814</td>\n",
       "      <td>150.287805</td>\n",
       "      <td>122.363184</td>\n",
       "      <td>...</td>\n",
       "      <td>76.980296</td>\n",
       "      <td>44.884422</td>\n",
       "      <td>127.060000</td>\n",
       "      <td>139.200000</td>\n",
       "      <td>53.112821</td>\n",
       "      <td>111.855000</td>\n",
       "      <td>157.480769</td>\n",
       "      <td>167.566327</td>\n",
       "      <td>105.402913</td>\n",
       "      <td>113.311558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195412 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             27_gpt-3.5-turbo  12_vicuna-7b  13_chatglm-6b  1_alpaca-13b  \\\n",
       "00                      0.000      0.000443       0.000303      0.000000   \n",
       "000                     0.000      0.000000       0.000000      0.000261   \n",
       "0000                    0.000      0.000000       0.000000      0.000000   \n",
       "00000                   0.000      0.000000       0.000000      0.000000   \n",
       "000000                  0.000      0.000000       0.000000      0.000000   \n",
       "...                       ...           ...            ...           ...   \n",
       "ｗｅｅｋｓ                   0.000      0.000000       0.000000      0.000000   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                  0.000      0.000000       0.000000      0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻              0.000      0.000000       0.000000      0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀             0.000      0.000000       0.000000      0.000000   \n",
       " length               106.535    113.361809     148.495000     38.325000   \n",
       "\n",
       "             8_mpt-7b-chat  15_stablelm-tuned-alpha-7b    12_gpt-4  \\\n",
       "00                0.000000                    0.000000    0.000071   \n",
       "000               0.000061                    0.000132    0.000000   \n",
       "0000              0.000000                    0.000000    0.000000   \n",
       "00000             0.000000                    0.000000    0.000000   \n",
       "000000            0.000000                    0.000044    0.000000   \n",
       "...                    ...                         ...         ...   \n",
       "ｗｅｅｋｓ             0.000000                    0.000000    0.000000   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲            0.000000                    0.000000    0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻        0.000000                    0.000000    0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀       0.000000                    0.000000    0.000000   \n",
       " length          83.795918                  111.465686  142.040404   \n",
       "\n",
       "             25_oasst-pythia-12b  9_claude-v1  30_koala-13b  ...  \\\n",
       "00                      0.000000     0.000325      0.000000  ...   \n",
       "000                     0.000123     0.000909      0.000244  ...   \n",
       "0000                    0.000000     0.000000      0.000000  ...   \n",
       "00000                   0.000000     0.000000      0.000000  ...   \n",
       "000000                  0.000000     0.000000      0.000000  ...   \n",
       "...                          ...          ...           ...  ...   \n",
       "ｗｅｅｋｓ                   0.000000     0.000000      0.000000  ...   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                  0.000000     0.000000      0.000000  ...   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻              0.000000     0.000000      0.000000  ...   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀             0.000000     0.000000      0.000000  ...   \n",
       " length                81.562814   150.287805    122.363184  ...   \n",
       "\n",
       "             4_gpt4all-13b-snoozy  3_llama-13b  32_vicuna-13b  \\\n",
       "00                       0.000000     0.000336       0.000000   \n",
       "000                      0.000192     0.000112       0.000039   \n",
       "0000                     0.000000     0.000000       0.000000   \n",
       "00000                    0.000000     0.000000       0.000000   \n",
       "000000                   0.000000     0.000000       0.000000   \n",
       "...                           ...          ...            ...   \n",
       "ｗｅｅｋｓ                    0.000000     0.000000       0.000000   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                   0.000000     0.000000       0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻               0.000000     0.000000       0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀              0.000000     0.000000       0.000000   \n",
       " length                 76.980296    44.884422     127.060000   \n",
       "\n",
       "             4_claude-instant-v1  9_dolly-v2-12b  4_RWKV-4-Raven-14B  \\\n",
       "00                      0.000000        0.000000            0.000000   \n",
       "000                     0.000111        0.000290            0.000179   \n",
       "0000                    0.000000        0.000000            0.000000   \n",
       "00000                   0.000000        0.000000            0.000000   \n",
       "000000                  0.000000        0.000000            0.000000   \n",
       "...                          ...             ...                 ...   \n",
       "ｗｅｅｋｓ                   0.000000        0.000000            0.000000   \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲                  0.000000        0.000000            0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻              0.000000        0.000000            0.000000   \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀             0.000000        0.000000            0.000000   \n",
       " length               139.200000       53.112821          111.855000   \n",
       "\n",
       "             0_guanaco-33b    8_palm-2  17_fastchat-t5-3b  5_wizardlm-13b  \n",
       "00                0.000000    0.000061           0.000046        0.000000  \n",
       "000               0.000031    0.000183           0.000092        0.000089  \n",
       "0000              0.000000    0.000000           0.000000        0.000000  \n",
       "00000             0.000000    0.000000           0.000000        0.000000  \n",
       "000000            0.000000    0.000000           0.000000        0.000000  \n",
       "...                    ...         ...                ...             ...  \n",
       "ｗｅｅｋｓ             0.000000    0.000000           0.000000        0.000000  \n",
       "𝘀𝗶𝗺𝗽𝗹𝗲            0.000000    0.000000           0.000000        0.000000  \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻        0.000000    0.000000           0.000000        0.000000  \n",
       "𝘀𝘂𝗽𝗲𝗿𝗵𝘂𝗺𝗮𝗻𝘀       0.000000    0.000000           0.000000        0.000000  \n",
       " length         157.480769  167.566327         105.402913      113.311558  \n",
       "\n",
       "[195412 rows x 21 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "training_dataset, holdout_dataset = holdout(batch_dataset, list(set(authors)))\n",
    "\n",
    "holdout_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25907064-528f-42f4-b404-7ee2b8d2ad6e",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2d359f4a-e989-4fed-834c-ba93a63c1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dataset = training_dataset[[name for name in training_dataset.columns if 'human' in name]].copy()\n",
    "llm_dataset = training_dataset[[name for name in training_dataset.columns if not 'human' in name]].copy()\n",
    "bruh = llm_dataset[llm_dataset.ne(0).sum(axis=1) == llm_dataset.shape[1]]\n",
    "\n",
    "#training_dataset\n",
    "\n",
    "# Now put `human_dataset` back into `training_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b1370b65-7558-47eb-921e-1246c3f88bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_chatglm-6b</th>\n",
       "      <th>0_koala-13b</th>\n",
       "      <th>0_oasst-pythia-12b</th>\n",
       "      <th>0_alpaca-13b</th>\n",
       "      <th>0_vicuna-13b</th>\n",
       "      <th>0_dolly-v2-12b</th>\n",
       "      <th>0_stablelm-tuned-alpha-7b</th>\n",
       "      <th>0_llama-13b</th>\n",
       "      <th>1_vicuna-13b</th>\n",
       "      <th>1_koala-13b</th>\n",
       "      <th>...</th>\n",
       "      <th>187_human</th>\n",
       "      <th>188_human</th>\n",
       "      <th>189_human</th>\n",
       "      <th>190_human</th>\n",
       "      <th>191_human</th>\n",
       "      <th>192_human</th>\n",
       "      <th>193_human</th>\n",
       "      <th>194_human</th>\n",
       "      <th>195_human</th>\n",
       "      <th>196_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.005965</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.000722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.010103</td>\n",
       "      <td>0.012080</td>\n",
       "      <td>0.010066</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.010108</td>\n",
       "      <td>0.013492</td>\n",
       "      <td>0.013432</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008199</td>\n",
       "      <td>0.017576</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>0.013485</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.016498</td>\n",
       "      <td>0.013610</td>\n",
       "      <td>0.013149</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.003007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.000962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>158.492462</td>\n",
       "      <td>121.745000</td>\n",
       "      <td>99.839196</td>\n",
       "      <td>43.432161</td>\n",
       "      <td>111.603015</td>\n",
       "      <td>62.216495</td>\n",
       "      <td>121.950739</td>\n",
       "      <td>53.125628</td>\n",
       "      <td>127.155000</td>\n",
       "      <td>130.310000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.482412</td>\n",
       "      <td>19.060000</td>\n",
       "      <td>24.115578</td>\n",
       "      <td>19.280000</td>\n",
       "      <td>33.865000</td>\n",
       "      <td>24.366834</td>\n",
       "      <td>23.145000</td>\n",
       "      <td>37.452261</td>\n",
       "      <td>22.035000</td>\n",
       "      <td>41.570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0_chatglm-6b  0_koala-13b  0_oasst-pythia-12b  0_alpaca-13b  \\\n",
       "10           0.000698     0.000945            0.000705      0.000463   \n",
       "about        0.001776     0.001068            0.001309      0.000810   \n",
       "after        0.000317     0.000205            0.000554      0.000231   \n",
       "all          0.001237     0.001068            0.001107      0.001620   \n",
       "also         0.003075     0.002998            0.002718      0.004165   \n",
       "...               ...          ...                 ...           ...   \n",
       "world        0.000919     0.001027            0.000856      0.001620   \n",
       "would        0.000476     0.001930            0.001057      0.003008   \n",
       "you          0.009480     0.010103            0.012080      0.010066   \n",
       "your         0.003234     0.004271            0.006845      0.003587   \n",
       " length    158.492462   121.745000           99.839196     43.432161   \n",
       "\n",
       "         0_vicuna-13b  0_dolly-v2-12b  0_stablelm-tuned-alpha-7b  0_llama-13b  \\\n",
       "10           0.000630        0.000994                   0.000687     0.001040   \n",
       "about        0.002116        0.001160                   0.003070     0.002459   \n",
       "after        0.000180        0.000580                   0.000404     0.000095   \n",
       "all          0.000856        0.000994                   0.002101     0.001797   \n",
       "also         0.003062        0.002486                   0.002747     0.002838   \n",
       "...               ...             ...                        ...          ...   \n",
       "world        0.001396        0.000994                   0.001656     0.002459   \n",
       "would        0.001891        0.001740                   0.002141     0.001419   \n",
       "you          0.008240        0.010108                   0.013492     0.013432   \n",
       "your         0.004233        0.002900                   0.004039     0.003689   \n",
       " length    111.603015       62.216495                 121.950739    53.125628   \n",
       "\n",
       "         1_vicuna-13b  1_koala-13b  ...  187_human  188_human  189_human  \\\n",
       "10           0.000472     0.000806  ...   0.001238   0.000262   0.001459   \n",
       "about        0.001769     0.002110  ...   0.001856   0.003148   0.005209   \n",
       "after        0.000708     0.000345  ...   0.000619   0.000262   0.000417   \n",
       "all          0.000708     0.001189  ...   0.003249   0.001574   0.001459   \n",
       "also         0.002713     0.003108  ...   0.000309   0.001049   0.000417   \n",
       "...               ...          ...  ...        ...        ...        ...   \n",
       "world        0.001455     0.001650  ...   0.000464   0.000787   0.000417   \n",
       "would        0.001455     0.001919  ...   0.001238   0.001049   0.003751   \n",
       "you          0.008965     0.009247  ...   0.008199   0.017576   0.012294   \n",
       "your         0.003146     0.005986  ...   0.001547   0.002099   0.003126   \n",
       " length    127.155000   130.310000  ...  32.482412  19.060000  24.115578   \n",
       "\n",
       "         190_human  191_human  192_human  193_human  194_human  195_human  \\\n",
       "10        0.001037   0.001772   0.001856   0.002592   0.001610   0.000227   \n",
       "about     0.005965   0.002658   0.005568   0.003672   0.004428   0.002496   \n",
       "after     0.000259   0.001624   0.000825   0.000864   0.005233   0.000227   \n",
       "all       0.002075   0.002805   0.002475   0.002592   0.002147   0.001815   \n",
       "also      0.000259   0.000886   0.000412   0.001080   0.001342   0.000908   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "world     0.000259   0.000000   0.000825   0.001944   0.000000   0.000681   \n",
       "would     0.003112   0.002362   0.003300   0.001080   0.001476   0.002042   \n",
       "you       0.013485   0.005758   0.016498   0.013610   0.013149   0.010211   \n",
       "your      0.002334   0.002658   0.003300   0.006697   0.002549   0.003177   \n",
       " length  19.280000  33.865000  24.366834  23.145000  37.452261  22.035000   \n",
       "\n",
       "         196_human  \n",
       "10        0.000481  \n",
       "about     0.001443  \n",
       "after     0.000601  \n",
       "all       0.000241  \n",
       "also      0.000241  \n",
       "...            ...  \n",
       "world     0.000361  \n",
       "would     0.000722  \n",
       "you       0.003007  \n",
       "your      0.000962  \n",
       " length  41.570000  \n",
       "\n",
       "[122 rows x 591 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.merge(bruh, human_dataset, left_index=True, right_index=True, validate='1:1')\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f3526-9fc8-45a2-a811-eda815e44805",
   "metadata": {},
   "source": [
    "# Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2bc08701-39eb-4c44-a290-782981174e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Homeworkace\\AppData\\Local\\Temp\\ipykernel_3508\\2290679722.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_tokens_only.loc[feature] -= features_mean[feature]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatglm-6b</th>\n",
       "      <th>koala-13b</th>\n",
       "      <th>human</th>\n",
       "      <th>oasst-pythia-12b</th>\n",
       "      <th>alpaca-13b</th>\n",
       "      <th>vicuna-13b</th>\n",
       "      <th>dolly-v2-12b</th>\n",
       "      <th>stablelm-tuned-alpha-7b</th>\n",
       "      <th>llama-13b</th>\n",
       "      <th>fastchat-t5-3b</th>\n",
       "      <th>...</th>\n",
       "      <th>gpt-4</th>\n",
       "      <th>RWKV-4-Raven-14B</th>\n",
       "      <th>claude-v1</th>\n",
       "      <th>mpt-7b-chat</th>\n",
       "      <th>palm-2</th>\n",
       "      <th>claude-instant-v1</th>\n",
       "      <th>vicuna-7b</th>\n",
       "      <th>wizardlm-13b</th>\n",
       "      <th>gpt4all-13b-snoozy</th>\n",
       "      <th>guanaco-33b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>04</th>\n",
       "      <td>-1.659460e-05</td>\n",
       "      <td>-1.370296e-05</td>\n",
       "      <td>1.677537e-04</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-8.678607e-06</td>\n",
       "      <td>-1.157621e-05</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.315954e-05</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-1.371759e-05</td>\n",
       "      <td>-1.606912e-05</td>\n",
       "      <td>-8.426521e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-3.784737e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>-1.010447e-05</td>\n",
       "      <td>-1.778170e-05</td>\n",
       "      <td>1.316913e-04</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-2.346519e-07</td>\n",
       "      <td>-9.942558e-06</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.413636e-05</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-6.344560e-06</td>\n",
       "      <td>-1.330445e-05</td>\n",
       "      <td>1.031573e-05</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-3.276912e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10th</th>\n",
       "      <td>-1.977722e-06</td>\n",
       "      <td>6.398017e-06</td>\n",
       "      <td>3.594102e-06</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>1.979629e-05</td>\n",
       "      <td>-7.444931e-06</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.040310e-06</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-5.062149e-07</td>\n",
       "      <td>-7.434545e-06</td>\n",
       "      <td>-5.870088e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-1.194823e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-7.249268e-06</td>\n",
       "      <td>-3.510167e-06</td>\n",
       "      <td>-1.015076e-06</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-5.275304e-06</td>\n",
       "      <td>-3.614247e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>8.572468e-06</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-2.378161e-06</td>\n",
       "      <td>-2.160005e-07</td>\n",
       "      <td>-3.165229e-06</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>2.435276e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-7.138689e-06</td>\n",
       "      <td>-9.082713e-06</td>\n",
       "      <td>2.128950e-05</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-9.173066e-06</td>\n",
       "      <td>-4.725128e-06</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.262665e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-7.955585e-06</td>\n",
       "      <td>-1.259551e-05</td>\n",
       "      <td>-1.103106e-05</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-5.910486e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xml</th>\n",
       "      <td>2.507718e-06</td>\n",
       "      <td>-1.130076e-05</td>\n",
       "      <td>-1.435235e-05</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-2.744408e-05</td>\n",
       "      <td>-4.982951e-06</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.014111e-05</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>1.339616e-04</td>\n",
       "      <td>6.617769e-05</td>\n",
       "      <td>-2.322393e-05</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5.420947e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yield</th>\n",
       "      <td>-4.068175e-07</td>\n",
       "      <td>-7.484136e-09</td>\n",
       "      <td>1.273093e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-6.449536e-06</td>\n",
       "      <td>-5.969662e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>9.467603e-07</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>1.826071e-05</td>\n",
       "      <td>-2.555835e-06</td>\n",
       "      <td>-6.197449e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-7.155021e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yo</th>\n",
       "      <td>-2.264166e-05</td>\n",
       "      <td>7.432457e-05</td>\n",
       "      <td>1.354101e-07</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-3.063821e-05</td>\n",
       "      <td>8.175055e-06</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.124439e-05</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-2.087585e-05</td>\n",
       "      <td>-3.009259e-05</td>\n",
       "      <td>-3.460627e-05</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-2.900692e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoga</th>\n",
       "      <td>8.011033e-06</td>\n",
       "      <td>2.336677e-05</td>\n",
       "      <td>-1.242676e-05</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-2.040026e-06</td>\n",
       "      <td>1.009045e-07</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>2.365800e-05</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-4.181869e-06</td>\n",
       "      <td>-4.371233e-06</td>\n",
       "      <td>1.247842e-05</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.231293e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yours</th>\n",
       "      <td>-6.026831e-06</td>\n",
       "      <td>-2.836012e-06</td>\n",
       "      <td>9.018725e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-4.073005e-06</td>\n",
       "      <td>-7.508916e-07</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.319103e-07</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>1.087489e-05</td>\n",
       "      <td>-2.981767e-06</td>\n",
       "      <td>1.471457e-07</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-8.104255e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1425 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         chatglm-6b     koala-13b         human  oasst-pythia-12b  \\\n",
       "04    -1.659460e-05 -1.370296e-05  1.677537e-04          0.000006   \n",
       "07    -1.010447e-05 -1.778170e-05  1.316913e-04         -0.000004   \n",
       "10th  -1.977722e-06  6.398017e-06  3.594102e-06         -0.000001   \n",
       "121   -7.249268e-06 -3.510167e-06 -1.015076e-06         -0.000006   \n",
       "123   -7.138689e-06 -9.082713e-06  2.128950e-05          0.000002   \n",
       "...             ...           ...           ...               ...   \n",
       "xml    2.507718e-06 -1.130076e-05 -1.435235e-05         -0.000020   \n",
       "yield -4.068175e-07 -7.484136e-09  1.273093e-05          0.000004   \n",
       "yo    -2.264166e-05  7.432457e-05  1.354101e-07         -0.000019   \n",
       "yoga   8.011033e-06  2.336677e-05 -1.242676e-05          0.000010   \n",
       "yours -6.026831e-06 -2.836012e-06  9.018725e-06         -0.000002   \n",
       "\n",
       "         alpaca-13b    vicuna-13b  dolly-v2-12b  stablelm-tuned-alpha-7b  \\\n",
       "04    -8.678607e-06 -1.157621e-05      0.000011                -0.000013   \n",
       "07    -2.346519e-07 -9.942558e-06      0.000063                -0.000002   \n",
       "10th   1.979629e-05 -7.444931e-06     -0.000007                 0.000014   \n",
       "121   -5.275304e-06 -3.614247e-06     -0.000004                -0.000007   \n",
       "123   -9.173066e-06 -4.725128e-06      0.000014                -0.000009   \n",
       "...             ...           ...           ...                      ...   \n",
       "xml   -2.744408e-05 -4.982951e-06     -0.000020                -0.000007   \n",
       "yield -6.449536e-06 -5.969662e-06      0.000002                 0.000013   \n",
       "yo    -3.063821e-05  8.175055e-06      0.000022                -0.000014   \n",
       "yoga  -2.040026e-06  1.009045e-07     -0.000018                -0.000002   \n",
       "yours -4.073005e-06 -7.508916e-07      0.000019                 0.000019   \n",
       "\n",
       "       llama-13b  fastchat-t5-3b  ...         gpt-4  RWKV-4-Raven-14B  \\\n",
       "04     -0.000013       -0.000021  ... -1.315954e-05         -0.000016   \n",
       "07     -0.000012       -0.000018  ... -1.413636e-05         -0.000016   \n",
       "10th    0.000018       -0.000002  ... -3.040310e-06         -0.000006   \n",
       "121     0.000029       -0.000002  ...  8.572468e-06         -0.000005   \n",
       "123     0.000036        0.000023  ... -2.262665e-06          0.000008   \n",
       "...          ...             ...  ...           ...               ...   \n",
       "xml    -0.000028       -0.000025  ... -1.014111e-05         -0.000012   \n",
       "yield  -0.000003       -0.000018  ...  9.467603e-07          0.000011   \n",
       "yo      0.000246       -0.000032  ... -2.124439e-05          0.000008   \n",
       "yoga   -0.000010        0.000012  ...  2.365800e-05         -0.000012   \n",
       "yours  -0.000012       -0.000005  ... -1.319103e-07         -0.000001   \n",
       "\n",
       "       claude-v1  mpt-7b-chat    palm-2  claude-instant-v1     vicuna-7b  \\\n",
       "04     -0.000009    -0.000003 -0.000009      -1.371759e-05 -1.606912e-05   \n",
       "07     -0.000017    -0.000010 -0.000016      -6.344560e-06 -1.330445e-05   \n",
       "10th    0.000002    -0.000008 -0.000006      -5.062149e-07 -7.434545e-06   \n",
       "121    -0.000006    -0.000006 -0.000009      -2.378161e-06 -2.160005e-07   \n",
       "123    -0.000002    -0.000010  0.000003      -7.955585e-06 -1.259551e-05   \n",
       "...          ...          ...       ...                ...           ...   \n",
       "xml     0.000005    -0.000035 -0.000016       1.339616e-04  6.617769e-05   \n",
       "yield   0.000017    -0.000005 -0.000016       1.826071e-05 -2.555835e-06   \n",
       "yo     -0.000016    -0.000010 -0.000025      -2.087585e-05 -3.009259e-05   \n",
       "yoga    0.000003    -0.000004 -0.000008      -4.181869e-06 -4.371233e-06   \n",
       "yours  -0.000004    -0.000009 -0.000006       1.087489e-05 -2.981767e-06   \n",
       "\n",
       "       wizardlm-13b  gpt4all-13b-snoozy   guanaco-33b  \n",
       "04    -8.426521e-06            0.000012 -3.784737e-06  \n",
       "07     1.031573e-05           -0.000020 -3.276912e-06  \n",
       "10th  -5.870088e-06            0.000012 -1.194823e-05  \n",
       "121   -3.165229e-06            0.000015  2.435276e-05  \n",
       "123   -1.103106e-05           -0.000017 -5.910486e-06  \n",
       "...             ...                 ...           ...  \n",
       "xml   -2.322393e-05            0.000005  5.420947e-05  \n",
       "yield -6.197449e-06           -0.000010 -7.155021e-06  \n",
       "yo    -3.460627e-05           -0.000026 -2.900692e-05  \n",
       "yoga   1.247842e-05           -0.000002 -1.231293e-05  \n",
       "yours  1.471457e-07           -0.000004 -8.104255e-07  \n",
       "\n",
       "[1425 rows x 21 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_mean = {feature: np.mean(common_tokens_only.loc[feature]) for feature in common_tokens_only.index}\n",
    "for feature in common_tokens_only.index :\n",
    "    common_tokens_only.loc[feature] -= features_mean[feature]\n",
    "\n",
    "common_tokens_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cac452-bea1-486b-a808-5c7598fec34a",
   "metadata": {},
   "source": [
    "# Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "600abda5-7eb3-49bc-993b-7a6112b50b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Homeworkace\\AppData\\Local\\Temp\\ipykernel_3508\\1559195010.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_tokens_only.loc[feature] /= features_std[feature]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatglm-6b</th>\n",
       "      <th>koala-13b</th>\n",
       "      <th>human</th>\n",
       "      <th>oasst-pythia-12b</th>\n",
       "      <th>alpaca-13b</th>\n",
       "      <th>vicuna-13b</th>\n",
       "      <th>dolly-v2-12b</th>\n",
       "      <th>stablelm-tuned-alpha-7b</th>\n",
       "      <th>llama-13b</th>\n",
       "      <th>fastchat-t5-3b</th>\n",
       "      <th>...</th>\n",
       "      <th>gpt-4</th>\n",
       "      <th>RWKV-4-Raven-14B</th>\n",
       "      <th>claude-v1</th>\n",
       "      <th>mpt-7b-chat</th>\n",
       "      <th>palm-2</th>\n",
       "      <th>claude-instant-v1</th>\n",
       "      <th>vicuna-7b</th>\n",
       "      <th>wizardlm-13b</th>\n",
       "      <th>gpt4all-13b-snoozy</th>\n",
       "      <th>guanaco-33b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>04</th>\n",
       "      <td>-0.431668</td>\n",
       "      <td>-0.356449</td>\n",
       "      <td>4.363698</td>\n",
       "      <td>0.146259</td>\n",
       "      <td>-0.225753</td>\n",
       "      <td>-0.301126</td>\n",
       "      <td>0.274759</td>\n",
       "      <td>-0.331525</td>\n",
       "      <td>-0.337875</td>\n",
       "      <td>-0.535411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342313</td>\n",
       "      <td>-0.425501</td>\n",
       "      <td>-0.242503</td>\n",
       "      <td>-0.086902</td>\n",
       "      <td>-0.227190</td>\n",
       "      <td>-0.356829</td>\n",
       "      <td>-0.417998</td>\n",
       "      <td>-0.219195</td>\n",
       "      <td>0.308310</td>\n",
       "      <td>-0.098451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>-0.296628</td>\n",
       "      <td>-0.522002</td>\n",
       "      <td>3.865948</td>\n",
       "      <td>-0.127749</td>\n",
       "      <td>-0.006888</td>\n",
       "      <td>-0.291875</td>\n",
       "      <td>1.848830</td>\n",
       "      <td>-0.052441</td>\n",
       "      <td>-0.366397</td>\n",
       "      <td>-0.516423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414989</td>\n",
       "      <td>-0.465286</td>\n",
       "      <td>-0.506684</td>\n",
       "      <td>-0.285628</td>\n",
       "      <td>-0.473377</td>\n",
       "      <td>-0.186252</td>\n",
       "      <td>-0.390567</td>\n",
       "      <td>0.302830</td>\n",
       "      <td>-0.589324</td>\n",
       "      <td>-0.096197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10th</th>\n",
       "      <td>-0.219408</td>\n",
       "      <td>0.709794</td>\n",
       "      <td>0.398729</td>\n",
       "      <td>-0.162643</td>\n",
       "      <td>2.196195</td>\n",
       "      <td>-0.825938</td>\n",
       "      <td>-0.749657</td>\n",
       "      <td>1.572942</td>\n",
       "      <td>2.044318</td>\n",
       "      <td>-0.223530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337291</td>\n",
       "      <td>-0.622410</td>\n",
       "      <td>0.235980</td>\n",
       "      <td>-0.942968</td>\n",
       "      <td>-0.668273</td>\n",
       "      <td>-0.056159</td>\n",
       "      <td>-0.824786</td>\n",
       "      <td>-0.651226</td>\n",
       "      <td>1.373226</td>\n",
       "      <td>-1.325533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.715568</td>\n",
       "      <td>-0.346485</td>\n",
       "      <td>-0.100197</td>\n",
       "      <td>-0.567509</td>\n",
       "      <td>-0.520720</td>\n",
       "      <td>-0.356759</td>\n",
       "      <td>-0.400016</td>\n",
       "      <td>-0.654511</td>\n",
       "      <td>2.835518</td>\n",
       "      <td>-0.177021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846180</td>\n",
       "      <td>-0.495333</td>\n",
       "      <td>-0.634532</td>\n",
       "      <td>-0.572015</td>\n",
       "      <td>-0.912404</td>\n",
       "      <td>-0.234746</td>\n",
       "      <td>-0.021321</td>\n",
       "      <td>-0.312437</td>\n",
       "      <td>1.488828</td>\n",
       "      <td>2.403837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.541378</td>\n",
       "      <td>-0.688808</td>\n",
       "      <td>1.614537</td>\n",
       "      <td>0.159875</td>\n",
       "      <td>-0.695660</td>\n",
       "      <td>-0.358341</td>\n",
       "      <td>1.064464</td>\n",
       "      <td>-0.703104</td>\n",
       "      <td>2.733777</td>\n",
       "      <td>1.715756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171594</td>\n",
       "      <td>0.625077</td>\n",
       "      <td>-0.123337</td>\n",
       "      <td>-0.774479</td>\n",
       "      <td>0.200138</td>\n",
       "      <td>-0.603329</td>\n",
       "      <td>-0.955209</td>\n",
       "      <td>-0.836565</td>\n",
       "      <td>-1.297514</td>\n",
       "      <td>-0.448235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xml</th>\n",
       "      <td>0.065113</td>\n",
       "      <td>-0.293423</td>\n",
       "      <td>-0.372658</td>\n",
       "      <td>-0.510391</td>\n",
       "      <td>-0.712583</td>\n",
       "      <td>-0.129382</td>\n",
       "      <td>-0.514301</td>\n",
       "      <td>-0.172432</td>\n",
       "      <td>-0.721470</td>\n",
       "      <td>-0.660726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263313</td>\n",
       "      <td>-0.315248</td>\n",
       "      <td>0.141204</td>\n",
       "      <td>-0.918644</td>\n",
       "      <td>-0.405883</td>\n",
       "      <td>3.478302</td>\n",
       "      <td>1.718298</td>\n",
       "      <td>-0.603007</td>\n",
       "      <td>0.134076</td>\n",
       "      <td>1.407544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yield</th>\n",
       "      <td>-0.041512</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>1.299084</td>\n",
       "      <td>0.444663</td>\n",
       "      <td>-0.658121</td>\n",
       "      <td>-0.609154</td>\n",
       "      <td>0.245905</td>\n",
       "      <td>1.326353</td>\n",
       "      <td>-0.323061</td>\n",
       "      <td>-1.872844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096609</td>\n",
       "      <td>1.145220</td>\n",
       "      <td>1.717828</td>\n",
       "      <td>-0.465320</td>\n",
       "      <td>-1.671329</td>\n",
       "      <td>1.863352</td>\n",
       "      <td>-0.260801</td>\n",
       "      <td>-0.632398</td>\n",
       "      <td>-1.045410</td>\n",
       "      <td>-0.730110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yo</th>\n",
       "      <td>-0.376003</td>\n",
       "      <td>1.234283</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>-0.313584</td>\n",
       "      <td>-0.508799</td>\n",
       "      <td>0.135760</td>\n",
       "      <td>0.373543</td>\n",
       "      <td>-0.227595</td>\n",
       "      <td>4.091339</td>\n",
       "      <td>-0.533455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352799</td>\n",
       "      <td>0.126980</td>\n",
       "      <td>-0.270827</td>\n",
       "      <td>-0.173829</td>\n",
       "      <td>-0.410719</td>\n",
       "      <td>-0.346678</td>\n",
       "      <td>-0.499738</td>\n",
       "      <td>-0.574695</td>\n",
       "      <td>-0.440035</td>\n",
       "      <td>-0.481708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoga</th>\n",
       "      <td>0.723058</td>\n",
       "      <td>2.109033</td>\n",
       "      <td>-1.121612</td>\n",
       "      <td>0.906206</td>\n",
       "      <td>-0.184128</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>-1.616724</td>\n",
       "      <td>-0.201847</td>\n",
       "      <td>-0.931317</td>\n",
       "      <td>1.072963</td>\n",
       "      <td>...</td>\n",
       "      <td>2.135320</td>\n",
       "      <td>-1.044680</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>-0.371741</td>\n",
       "      <td>-0.725507</td>\n",
       "      <td>-0.377446</td>\n",
       "      <td>-0.394538</td>\n",
       "      <td>1.126275</td>\n",
       "      <td>-0.152961</td>\n",
       "      <td>-1.111338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yours</th>\n",
       "      <td>-0.760389</td>\n",
       "      <td>-0.357812</td>\n",
       "      <td>1.137868</td>\n",
       "      <td>-0.192652</td>\n",
       "      <td>-0.513880</td>\n",
       "      <td>-0.094738</td>\n",
       "      <td>2.414367</td>\n",
       "      <td>2.440424</td>\n",
       "      <td>-1.515160</td>\n",
       "      <td>-0.575212</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016643</td>\n",
       "      <td>-0.182437</td>\n",
       "      <td>-0.449652</td>\n",
       "      <td>-1.080084</td>\n",
       "      <td>-0.767685</td>\n",
       "      <td>1.372056</td>\n",
       "      <td>-0.376201</td>\n",
       "      <td>0.018565</td>\n",
       "      <td>-0.492096</td>\n",
       "      <td>-0.102249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1425 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       chatglm-6b  koala-13b     human  oasst-pythia-12b  alpaca-13b  \\\n",
       "04      -0.431668  -0.356449  4.363698          0.146259   -0.225753   \n",
       "07      -0.296628  -0.522002  3.865948         -0.127749   -0.006888   \n",
       "10th    -0.219408   0.709794  0.398729         -0.162643    2.196195   \n",
       "121     -0.715568  -0.346485 -0.100197         -0.567509   -0.520720   \n",
       "123     -0.541378  -0.688808  1.614537          0.159875   -0.695660   \n",
       "...           ...        ...       ...               ...         ...   \n",
       "xml      0.065113  -0.293423 -0.372658         -0.510391   -0.712583   \n",
       "yield   -0.041512  -0.000764  1.299084          0.444663   -0.658121   \n",
       "yo      -0.376003   1.234283  0.002249         -0.313584   -0.508799   \n",
       "yoga     0.723058   2.109033 -1.121612          0.906206   -0.184128   \n",
       "yours   -0.760389  -0.357812  1.137868         -0.192652   -0.513880   \n",
       "\n",
       "       vicuna-13b  dolly-v2-12b  stablelm-tuned-alpha-7b  llama-13b  \\\n",
       "04      -0.301126      0.274759                -0.331525  -0.337875   \n",
       "07      -0.291875      1.848830                -0.052441  -0.366397   \n",
       "10th    -0.825938     -0.749657                 1.572942   2.044318   \n",
       "121     -0.356759     -0.400016                -0.654511   2.835518   \n",
       "123     -0.358341      1.064464                -0.703104   2.733777   \n",
       "...           ...           ...                      ...        ...   \n",
       "xml     -0.129382     -0.514301                -0.172432  -0.721470   \n",
       "yield   -0.609154      0.245905                 1.326353  -0.323061   \n",
       "yo       0.135760      0.373543                -0.227595   4.091339   \n",
       "yoga     0.009107     -1.616724                -0.201847  -0.931317   \n",
       "yours   -0.094738      2.414367                 2.440424  -1.515160   \n",
       "\n",
       "       fastchat-t5-3b  ...     gpt-4  RWKV-4-Raven-14B  claude-v1  \\\n",
       "04          -0.535411  ... -0.342313         -0.425501  -0.242503   \n",
       "07          -0.516423  ... -0.414989         -0.465286  -0.506684   \n",
       "10th        -0.223530  ... -0.337291         -0.622410   0.235980   \n",
       "121         -0.177021  ...  0.846180         -0.495333  -0.634532   \n",
       "123          1.715756  ... -0.171594          0.625077  -0.123337   \n",
       "...               ...  ...       ...               ...        ...   \n",
       "xml         -0.660726  ... -0.263313         -0.315248   0.141204   \n",
       "yield       -1.872844  ...  0.096609          1.145220   1.717828   \n",
       "yo          -0.533455  ... -0.352799          0.126980  -0.270827   \n",
       "yoga         1.072963  ...  2.135320         -1.044680   0.288889   \n",
       "yours       -0.575212  ... -0.016643         -0.182437  -0.449652   \n",
       "\n",
       "       mpt-7b-chat    palm-2  claude-instant-v1  vicuna-7b  wizardlm-13b  \\\n",
       "04       -0.086902 -0.227190          -0.356829  -0.417998     -0.219195   \n",
       "07       -0.285628 -0.473377          -0.186252  -0.390567      0.302830   \n",
       "10th     -0.942968 -0.668273          -0.056159  -0.824786     -0.651226   \n",
       "121      -0.572015 -0.912404          -0.234746  -0.021321     -0.312437   \n",
       "123      -0.774479  0.200138          -0.603329  -0.955209     -0.836565   \n",
       "...            ...       ...                ...        ...           ...   \n",
       "xml      -0.918644 -0.405883           3.478302   1.718298     -0.603007   \n",
       "yield    -0.465320 -1.671329           1.863352  -0.260801     -0.632398   \n",
       "yo       -0.173829 -0.410719          -0.346678  -0.499738     -0.574695   \n",
       "yoga     -0.371741 -0.725507          -0.377446  -0.394538      1.126275   \n",
       "yours    -1.080084 -0.767685           1.372056  -0.376201      0.018565   \n",
       "\n",
       "       gpt4all-13b-snoozy  guanaco-33b  \n",
       "04               0.308310    -0.098451  \n",
       "07              -0.589324    -0.096197  \n",
       "10th             1.373226    -1.325533  \n",
       "121              1.488828     2.403837  \n",
       "123             -1.297514    -0.448235  \n",
       "...                   ...          ...  \n",
       "xml              0.134076     1.407544  \n",
       "yield           -1.045410    -0.730110  \n",
       "yo              -0.440035    -0.481708  \n",
       "yoga            -0.152961    -1.111338  \n",
       "yours           -0.492096    -0.102249  \n",
       "\n",
       "[1425 rows x 21 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_std = {feature: np.std(common_tokens_only.loc[feature]) for feature in common_tokens_only.index}\n",
    "for feature in common_tokens_only.index :\n",
    "    common_tokens_only.loc[feature] /= features_std[feature]\n",
    "\n",
    "common_tokens_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e7a20af-01ee-43c1-9ebc-c68a09a547e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '_length'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcommon_tokens_only\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mstd(common_tokens_only\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_length\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum([np\u001b[38;5;241m.\u001b[39mstd(common_tokens_only\u001b[38;5;241m.\u001b[39mloc[feature]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m common_tokens_only\u001b[38;5;241m.\u001b[39mindex]))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1312\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1260\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4056\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4054\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[0;32m   4055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4056\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   4059\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '_length'"
     ]
    }
   ],
   "source": [
    "print(np.mean(common_tokens_only.loc['_length']))\n",
    "print(np.std(common_tokens_only.loc['_length']))\n",
    "\n",
    "print(np.sum([np.std(common_tokens_only.loc[feature]) for feature in common_tokens_only.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691cf28-c89d-4170-bf3a-5c07360d440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author_df.to_csv('bruh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0355b0-f917-492d-a455-6974b52b7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the two features\n",
    "x_token = 'zu'\n",
    "y_token = '_length'\n",
    "\n",
    "# Creating the scatter plot\n",
    "feature_graph_figure = plt.figure(figsize=(8, 6))\n",
    "feature_graph = feature_graph_figure.subplots()\n",
    "feature_graph.scatter(common_tokens_only.loc[x_token], common_tokens_only.loc[y_token], color='blue', alpha=0.7)\n",
    "feature_graph.set_xlabel(\"Feature '\" + x_token + \"'\")\n",
    "feature_graph.set_ylabel(\"Feature '\" + y_token + \"'\")\n",
    "feature_graph.grid(True)\n",
    "feature_graph_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828b582-fdd1-4580-b44d-2bb286d459a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration(A, num_iter=1000, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Power Iteration to compute the largest eigenvector of matrix A.\n",
    "    Args:\n",
    "        A (np.ndarray): Input symmetric matrix.\n",
    "        num_iter (int): Number of iterations.\n",
    "        tol (float): Tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "        eigenvector (np.ndarray): Approximation of the largest eigenvector.\n",
    "        eigenvalue (float): Corresponding eigenvalue.\n",
    "    \"\"\"\n",
    "    b = np.random.rand(A.shape[1])  # Random initial vector\n",
    "    b = b / np.linalg.norm(b)\n",
    "\n",
    "    error_meter = tqdm(position = 0, total=7, bar_format='{bar} | {postfix}')\n",
    "    for i in range(num_iter):\n",
    "        b_next = A @ b  # Matrix-vector multiplication\n",
    "        b_next = b_next / np.linalg.norm(b_next)  # Normalize\n",
    "        \n",
    "        # Check for convergence\n",
    "        error = np.linalg.norm(b_next - b)\n",
    "        b = b_next\n",
    "        error_meter.update(-np.log10(error) - error_meter.n)\n",
    "        error_meter.set_postfix_str('{:.2E}'.format(error) + ' of error after ' + str(i + 1) + ' iterations...')\n",
    "        if error < tol:\n",
    "            error_meter.close()\n",
    "            break\n",
    "\n",
    "    eigenvalue = b.T @ A @ b  # Rayleigh quotient for eigenvalue\n",
    "    return b, eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f551f-8f51-4519-b5c5-12f33b2acb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_power_iteration(data, num_components=2, num_iter=1000):\n",
    "    \"\"\"\n",
    "    PCA using Power Iteration to compute the principal components.\n",
    "    Args:\n",
    "        data (np.ndarray): Input data matrix (rows are samples, columns are features).\n",
    "        num_components (int): Number of principal components to compute.\n",
    "        num_iter (int): Number of iterations for power iteration.\n",
    "\n",
    "    Returns:\n",
    "        components (list): List of principal components (eigenvectors).\n",
    "        explained_variance (list): List of corresponding eigenvalues.\n",
    "    \"\"\"\n",
    "    # Step 1: Center the data\n",
    "    covariance_matrix = np.cov(data, rowvar=True, ddof = 0)\n",
    "    \n",
    "    # Compute covariance matrix manually\n",
    "    n = data.shape[1]  # Number of samples (columns in your case)\n",
    "    covariance_matrix = (data @ data.T) / n\n",
    "\n",
    "    components = []\n",
    "    explained_variance = []\n",
    "\n",
    "    for i in range(num_components):\n",
    "        # Step 2: Compute the largest eigenvector using power iteration\n",
    "        eigenvector, eigenvalue = power_iteration(covariance_matrix, num_iter=num_iter)\n",
    "        components.append(eigenvector)\n",
    "        explained_variance.append(eigenvalue)\n",
    "        \n",
    "        # Step 3: Deflate the covariance matrix\n",
    "        covariance_matrix -= eigenvalue * np.outer(eigenvector, eigenvector)\n",
    "        if np.linalg.norm(covariance_matrix, ord='fro') < 1e-10 :\n",
    "            print('Stopped as all variance has been extracted after computing ' + str(i + 1) + ' PCs.')\n",
    "            break\n",
    "        \n",
    "    return components, explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9e8da-7ddd-49f9-b056-24a83a5293f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "components, explained_variance = pca_power_iteration(common_tokens_only, num_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af60af-31bd-4f46-adfd-a36077e7072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(explained_variance))\n",
    "for variance in range(len(explained_variance)) :\n",
    "    print(np.sum(explained_variance[:variance + 1]))\n",
    "print(len(components[0]))\n",
    "print(np.sum([np.std(common_tokens_only.loc[feature]) for feature in common_tokens_only.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4938c0-7c70-493c-9ee8-b6c9d6b4255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the scatter plot\n",
    "feature_graph_figure = plt.figure(figsize=(8, 6))\n",
    "feature_graph = feature_graph_figure.subplots()\n",
    "coords = [[np.dot(common_tokens_only[author], components[0]), np.dot(common_tokens_only[author], components[1])] for author in common_tokens_only]\n",
    "feature_graph.scatter([coord[0] for coord in coords], [coord[1] for coord in coords], color='blue', alpha=0.7)\n",
    "for i, author in enumerate(common_tokens_only.columns):\n",
    "    feature_graph.annotate(author, coords[i])\n",
    "feature_graph.set_xlabel(\"PC1\")\n",
    "feature_graph.set_ylabel(\"PC2\")\n",
    "feature_graph.grid(True)\n",
    "feature_graph_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d94dd-9812-441b-8950-9a17016c1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(common_tokens_only['claude-v1'] - common_tokens_only['claude-instant-v1']))\n",
    "print(np.dot(common_tokens_only['claude-v1'], common_tokens_only['claude-instant-v1']))\n",
    "print(np.linalg.norm(common_tokens_only['claude-v1'] - common_tokens_only['alpaca-13b']))\n",
    "print(np.dot(common_tokens_only['claude-v1'], common_tokens_only['alpaca-13b']))\n",
    "print(np.linalg.norm(common_tokens_only['alpaca-13b'] - common_tokens_only['palm-2']))\n",
    "print(np.dot(common_tokens_only['alpaca-13b'], common_tokens_only['palm-2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ad049-fd16-4540-82e5-1f628aec7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(common_tokens_only['claude-v1'] - common_tokens_only['llama-13b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969bbcc-f625-46e0-9d44-7ae7b335421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
